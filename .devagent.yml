provider: ollama
# Default models (Ollama)
model: "llama3.2:3b"
planner_model: "llama3.2:3b"
developer_model: "llama3.2:3b"
reviewer_model: "llama3.1:8b-instruct-q4_K_M"
embed_model: "nomic-embed-text"

# OpenAI models (use with --provider openai)
# Uncomment to use OpenAI models by default:
# model: "gpt-4o-mini"
# planner_model: "gpt-4o-mini"
# developer_model: "gpt-4o-mini"
# reviewer_model: "gpt-4o"
# embed_model: "text-embedding-3-small"
ollama:
  host: "http://localhost:11434"
  params:
    temperature: 0.2
    top_p: 0.95
openai:
  # Use Ollama's OpenAI-compatible API (for embeddings)
  uri_base: "http://localhost:11434/v1"
  api_key: "ollama" # Dummy key for Ollama
  request_timeout: 600
  params:
    temperature: 0.2
    top_p: 0.95
  options:
    num_gpu: 0
    num_ctx: 2048
auto:
  max_iterations: 3
  dry_run: false
  require_tests_green: true
  confirmation_threshold: 0.7
  allowlist:
    - "app/**"
    - "lib/**"
    - "spec/**"
    - "config/**"
    - "db/**"
    - "src/**"
    - "bin/**"
    - "exe/**"
  denylist:
    - ".git/**"
    - "node_modules/**"
    - "log/**"
    - "tmp/**"
    - "dist/**"
    - "build/**"
    - ".env*"
    - "config/credentials*"
index:
  threads: 8
  globs:
    - "**/*.{rb,rbs,ru,erb,haml,slim,js,jsx,ts,tsx}"
  chunk_size: 1800
  overlap: 200
memory:
  short_term_turns: 20
